# -*- coding: utf-8 -*-
"""webscrap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xahP0qngVW6icNDrskriIr7TZ9hSxzS5
"""

import time
from requests import get
from bs4 import BeautifulSoup
from dateutil import parser

# Example BBC URL (Replace with actual article URL)
url = 'https://www.kevanlee.com/articles/writing-articles'

# Define headers with a User-Agent to mimic a browser request
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive'
}

# Retry function to handle 500 errors and retry a few times before failing
def scrape_bbc_article(url, retries=3):
    attempt = 0
    while attempt < retries:
        try:
            response = get(url, headers=headers)

            # If the response is successful
            if response.status_code == 200:
                print(f"Successfully fetched article: {url}")
                soup = BeautifulSoup(response.text, 'lxml')

                # Extract headline
                headline = soup.find('h1').text.strip()

                # Extract publish date
                try:
                    date = soup.find('time')['datetime']
                    publish_date = parser.parse(date).strftime('%Y-%m-%d')
                except:
                    publish_date = None

                # Extract author
                author = None
                try:
                    author = soup.find('span', {'class': 'byline__name'}).text.strip()
                except:
                    author = None

                # Extract article content
                article_content = ""
                try:
                    paragraphs = soup.find_all('p', {'class': 'ssrcss-1q0x1d5-Paragraph eq5iqo00'})
                    article_content = " ".join([para.text for para in paragraphs])
                except:
                    article_content = None

                # Print the extracted information
                print(f"Headline: {headline}")
                print(f"Author: {author if author else 'Not available'}")
                print(f"Publish Date: {publish_date if publish_date else 'Not available'}")
                print(f"Article Content: {article_content[:300]}...")  # Preview the article content

                return  # Exit after a successful fetch

            # If status code is not 200, print the error and retry
            elif response.status_code == 500:
                print(f"[ERROR] Server error (500). Retrying... ({attempt + 1}/{retries})")
                attempt += 1
                time.sleep(5)  # Wait before retrying

            else:
                print(f"[ERROR] Failed to fetch the article. Status code: {response.status_code}")
                break

        except Exception as e:
            print(f"[ERROR] Request failed: {e}. Retrying... ({attempt + 1}/{retries})")
            attempt += 1
            time.sleep(5)  # Wait before retrying

    print(f"[ERROR] Failed to fetch the article after {retries} retries.")

# Scrape the article
scrape_bbc_article(url)